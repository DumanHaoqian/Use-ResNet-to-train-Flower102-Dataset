{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ad381",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Tensor board'''\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision \n",
    "\n",
    "class ModleTrainer:\n",
    "    @staticmethod\n",
    "    def train(dataloader, model, loss_function, optimizer, scheduler, epoch_idx, device, log_interval, max_epoch, writer):\n",
    "        model.train()\n",
    "        num_cls = model.fc.out_features\n",
    "        confusion_matrix = np.zeros((num_cls, num_cls))\n",
    "        loss_sigma = []\n",
    "        loss_mean = 0\n",
    "        acc_avg = 0\n",
    "        \n",
    "        for i, data in enumerate(dataloader):\n",
    "            inputs, labels = data \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward & backward\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Loss items\n",
    "            loss_sigma.append(loss.item())\n",
    "            loss_mean = np.mean(loss_sigma)\n",
    "            \n",
    "            # Calculate confusion matrix\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            for j in range(len(labels)):\n",
    "                cate_i = labels[j].cpu().numpy()\n",
    "                pred_i = predicted[j].cpu().numpy()\n",
    "                confusion_matrix[cate_i, pred_i] += 1\n",
    "            \n",
    "            acc_avg = confusion_matrix.trace() / confusion_matrix.sum()\n",
    "            \n",
    "            # Record to TensorBoard\n",
    "            if i % log_interval == log_interval - 1:\n",
    "                # Log loss and accuracy\n",
    "                writer.add_scalar('Loss/train', loss_mean, epoch_idx * len(dataloader) + i)\n",
    "                writer.add_scalar('Accuracy/train', acc_avg, epoch_idx * len(dataloader) + i)\n",
    "                \n",
    "                # Log parameters and gradients\n",
    "                for name, param in model.named_parameters():\n",
    "                    writer.add_histogram(f'params/{name}', param.data.cpu().numpy(), epoch_idx * len(dataloader) + i)\n",
    "                    writer.add_histogram(f'grads/{name}', param.grad.data.cpu().numpy(), epoch_idx * len(dataloader) + i)\n",
    "                \n",
    "                print(\"Training: Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Accuracy:{:.2%}\".format(\n",
    "                    epoch_idx + 1, max_epoch, i + 1, len(dataloader), loss_mean, acc_avg))\n",
    "        \n",
    "        return loss_mean, acc_avg, confusion_matrix\n",
    "            \n",
    "    @staticmethod\n",
    "    def valid(dataloader, model, loss_function, device, epoch_idx, max_epoch, writer):\n",
    "        model.eval()\n",
    "        num_cls = model.fc.out_features\n",
    "        confusion_matrix = np.zeros((num_cls, num_cls))\n",
    "        loss_sigma = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(dataloader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                for j in range(len(labels)):\n",
    "                    cate_i = labels[j].cpu().numpy()\n",
    "                    pred_i = predicted[j].cpu().numpy()\n",
    "                    confusion_matrix[cate_i, pred_i] += 1\n",
    "                \n",
    "                acc_avg = confusion_matrix.trace() / confusion_matrix.sum()\n",
    "                loss_sigma.append(loss.item())\n",
    "        \n",
    "        print(\"Validing: Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Accuracy:{:.2%}\".format(\n",
    "            epoch_idx + 1, max_epoch, i + 1, len(dataloader), np.mean(loss_sigma), acc_avg))\n",
    "        \n",
    "        # Log validation loss and accuracy\n",
    "        writer.add_scalar('Loss/valid', np.mean(loss_sigma), epoch_idx)\n",
    "        writer.add_scalar('Accuracy/valid', acc_avg, epoch_idx)\n",
    "        \n",
    "        return np.mean(loss_sigma), acc_avg, confusion_matri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd76ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "from my_flower_dataset import Flowerdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc6449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=10086):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca30e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r\"train\"\n",
    "valid_dir = r\"valid\"\n",
    "batch_size =64\n",
    "max_epoch = 20\n",
    "num_cls = 102\n",
    "lr0 = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "milestones = [25, 35]\n",
    "decay_factor = 0.1\n",
    "norm_mean, norm_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "log_interval = 10  \n",
    "time_str = time.strftime(\"%Y%m%d\")\n",
    "output_dir = f\"outputs/{time_str}\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # (256, 256)区别  256：短边保持256  1920x1080 [1080->256 1920*(1080/256)]\n",
    "    transforms.RandomCrop(224),  # 模型最终的输入大小[224, 224]\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),  # 1)0-225 -> 0-1 float  2)HWC -> CHW  -> BCHW\n",
    "    transforms.Normalize(norm_mean, norm_std)  # 减去均值 除以方差\n",
    "])\n",
    "train_dataset = Flowerdata(img_dir=train_dir, transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c56f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # 0-225 -> 0-1 float HWC-> CHW   BCHW\n",
    "    transforms.Normalize(norm_mean, norm_std)  # 减去均值 除以方差\n",
    "])\n",
    "valid_dataset = Flowerdata(img_dir=valid_dir, transform=valid_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa8cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)  # 1000  fc\n",
    "in_features = model.fc.in_features\n",
    "fc = nn.Linear(in_features=in_features, out_features=num_cls)\n",
    "model.fc = fc\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"The device is : \"+f\"{device}\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1db501",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr0, momentum=momentum, weight_decay=weight_decay)\n",
    "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=decay_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b8974",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list=[]\n",
    "valid_loss_list=[]\n",
    "trainer=ModleTrainer()\n",
    "writer = SummaryWriter(log_dir='logs/')\n",
    "for epoch in range(max_epoch):\n",
    "    loss_train, acc_train, conf_mat_train= trainer.train(\n",
    "        train_loader, model, \n",
    "        loss_function=loss_fn, \n",
    "        optimizer=optimizer, \n",
    "        scheduler=lr_scheduler, \n",
    "        epoch_idx=epoch,\n",
    "        device=device,\n",
    "        log_interval=log_interval,\n",
    "        max_epoch=max_epoch,\n",
    "        writer=writer\n",
    "    )\n",
    "    train_loss_list.append(loss_train)\n",
    "    loss_valid, acc_valid, conf_mat_valid = trainer.valid(\n",
    "        valid_loader, \n",
    "        model, loss_fn, \n",
    "        device=device,\n",
    "        epoch_idx=epoch,\n",
    "        max_epoch=max_epoch,\n",
    "        writer=writer\n",
    "    )\n",
    "    valid_loss_list.append(loss_valid)\n",
    "    checkpoint = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "    }\n",
    "    torch.save(checkpoint, f\"{output_dir}/model.pth\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab9bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x1=np.arange(len(train_loss_list))\n",
    "y1=train_loss_list\n",
    "x2=np.arange(len(valid_loss_list))\n",
    "x2*(len(train_loss_list)/len(valid_loss_list))\n",
    "y2=valid_loss_list\n",
    "plt.title('Loss function')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss_value')\n",
    "plt.plot(x1,y1,color='blue',label='Train Loss')\n",
    "plt.plot(x2,y2,color='red',label='Valid Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4874ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59670155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from easydict import EasyDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# config\n",
    "cfg = EasyDict()\n",
    "cfg.num_cls = 102\n",
    "cfg.transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # 0-255 -> 0-1 float HWC-> CHW   BCHW\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # 减去均值 除以方差\n",
    "])\n",
    "\n",
    "\n",
    "model = models.resnet18()\n",
    "in_features = model.fc.in_features\n",
    "fc = nn.Linear(in_features=in_features, out_features=cfg.num_cls)\n",
    "model.fc = fc\n",
    "\n",
    "model_weights = os.path.join(\"outputs\", \"20240902\", \"model.pth\")\n",
    "checkpoint = torch.load(model_weights)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device=device)\n",
    "\n",
    "data_dir = r\"valid\"\n",
    "c = 0\n",
    "for img_name in os.listdir(data_dir):\n",
    "    if c == 10:\n",
    "        break\n",
    "    else:\n",
    "        img_path = os.path.join(data_dir, img_name)\n",
    "        img0 = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        img: torch.Tensor = cfg.transforms(img0)  # 0-1 CHW\n",
    "        img = img.unsqueeze(dim=0)  # 添加 batch 维度\n",
    "\n",
    "        # 推理\n",
    "        img = img.to(device=device)\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "\n",
    "        _, pred_label = torch.max(output, 1)\n",
    "\n",
    "        # 展示\n",
    "        plt.imshow(img0)\n",
    "        plt.axis('off')\n",
    "        plt.text(x=10, y=20, s=f'Pred Label: {int(pred_label.item())}', color='white', fontsize=12,\n",
    "                 bbox=dict(facecolor='black', alpha=0.5))\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"path: {img_path}, pred label: {int(pred_label.item())}\")\n",
    "        c += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python dl1",
   "language": "python",
   "name": "new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
